{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN and NLP-Detect a disaster in tweets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtWU64IPwR3uQH+cg4iPOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doo52oh/Kaggle/blob/master/RNN_and_NLP_Detect_a_disaster_in_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVw79gRBiItt",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "b794f2a0-455b-49bb-f632-99f60c67f3a0"
      },
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.38.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a49e5a89-26bb-403b-90da-abcdefd0b3bc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a49e5a89-26bb-403b-90da-abcdefd0b3bc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ohdddd\",\"key\":\"6d119e826d864d02a2b6057e0ef48b09\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDHCqFcbiwTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75618a42-ba1c-4d0f-e99c-a115ac5ecc7a"
      },
      "source": [
        "ls -1ha kaggle.json\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3fPvc9Ui6U2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1d5b0041-c99b-415e-c1ec-762c3c962768"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Permission Warning 이 일어나지 않도록 \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# 본인이 참가한 모든 대회 보기 \n",
        "!kaggle competitions list"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                               deadline             category            reward  teamCount  userHasEntered  \n",
            "------------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "digit-recognizer                                  2030-01-01 00:00:00  Getting Started  Knowledge       2801           False  \n",
            "titanic                                           2030-01-01 00:00:00  Getting Started  Knowledge      20466            True  \n",
            "house-prices-advanced-regression-techniques       2030-01-01 00:00:00  Getting Started  Knowledge       4886           False  \n",
            "connectx                                          2030-01-01 00:00:00  Getting Started  Knowledge        374           False  \n",
            "nlp-getting-started                               2030-01-01 00:00:00  Getting Started      Kudos       2351           False  \n",
            "competitive-data-science-predict-future-sales     2020-12-31 23:59:00  Playground           Kudos       6610            True  \n",
            "global-wheat-detection                            2020-08-04 23:59:00  Research           $15,000          8           False  \n",
            "hashcode-photo-slideshow                          2020-07-27 23:59:00  Playground       Knowledge         19           False  \n",
            "prostate-cancer-grade-assessment                  2020-07-22 23:59:00  Featured           $25,000        256           False  \n",
            "alaska2-image-steganalysis                        2020-07-20 23:59:00  Research           $25,000        194           False  \n",
            "halite                                            2020-06-30 23:59:00  Featured             Kudos          0           False  \n",
            "m5-forecasting-accuracy                           2020-06-30 23:59:00  Featured           $50,000       3379           False  \n",
            "m5-forecasting-uncertainty                        2020-06-30 23:59:00  Featured           $50,000        364           False  \n",
            "trends-assessment-prediction                      2020-06-29 23:59:00  Research           $25,000        238           False  \n",
            "jigsaw-multilingual-toxic-comment-classification  2020-06-22 23:59:00  Featured           $50,000        810           False  \n",
            "tweet-sentiment-extraction                        2020-06-16 23:59:00  Featured           $15,000        905           False  \n",
            "imet-2020-fgvc7                                   2020-05-28 23:59:00  Research         Knowledge         62           False  \n",
            "abstraction-and-reasoning-challenge               2020-05-27 23:59:00  Research           $20,000        744           False  \n",
            "imaterialist-fashion-2020-fgvc7                   2020-05-26 23:59:00  Research         Knowledge         35           False  \n",
            "iwildcam-2020-fgvc7                               2020-05-26 23:59:00  Research         Knowledge        101           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljsxCFJIjQB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "5d1a2212-9f90-410a-f720-4b6ffcc6d939"
      },
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/22.2k [00:00<?, ?B/s]\n",
            "100% 22.2k/22.2k [00:00<00:00, 8.62MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/411k [00:00<?, ?B/s]\n",
            "100% 411k/411k [00:00<00:00, 56.7MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/965k [00:00<?, ?B/s]\n",
            "100% 965k/965k [00:00<00:00, 64.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHTXFa9njYlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b4e2659-238e-447e-8306-013c6e82b6aa"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json  sample_data  sample_submission.csv  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkpz8kyHxuZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "c3987ecd-2862-4a9a-d2a6-d406dc3a28e1"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXry1uQYgtoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4865bc40-cf44-45cb-fc63-b03c38f6198f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_colwidth',-1)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OFiMLGojtGx",
        "colab_type": "text"
      },
      "source": [
        "Load Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZZAKkUchTQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "118a17dc-b5c7-4aa3-b07b-f427ddbd7a33"
      },
      "source": [
        "train = pd.read_csv('train.csv',\n",
        "                         usecols=['text','target'],\n",
        "                         dtype={'text':str, 'target':np.int64})\n",
        "\n",
        "len(train)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrJE2jEfiq50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "b1cd7358-9510-4d22-e4ae-231476009ad4"
      },
      "source": [
        "train['text'].head().values"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
              "       'Forest fire near La Ronge Sask. Canada',\n",
              "       \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
              "       '13,000 people receive #wildfires evacuation orders in California ',\n",
              "       'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRgEiNSln1A-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('test.csv',\n",
        "                   usecols=['text','id'],\n",
        "                   dtype={'text':str, 'id':str})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um3OM5YaoOHu",
        "colab_type": "text"
      },
      "source": [
        "** Mislabelled examples**\n",
        "\n",
        "There are a number of examples in the training dataset that are mislabelled. The keyword can b used to find these\n",
        "\n",
        "Thanks to Dmitri Kalyaevs whose notebook is where i found to do this : \n",
        "https://www.kaggle.com/dmitri9149/transformer-svm-semantically-identical-tweets\n",
        "\n",
        "\n",
        "\n",
        "1.   항목 추가\n",
        "2.   항목 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MTpO8DZoEme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "4f0b350e-b8af-450e-d329-7fed28a3c4fa"
      },
      "source": [
        "indices = [4415,4400,4399,4403,4397,4396,4394,4414,4393,4392,4404,4407,4420,4412,4408,4391,4405]\n",
        "train.loc[indices]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4415</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/xV3D9bPjHi #prebreak #best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4400</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/aAtt5aMnmD #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4399</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/nQiObcZKrT #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4403</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/qj3PVgaVN7 #prebreak #best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4397</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J5onxFwLAo #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4396</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/wvTPuRYx63 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4394</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cx6auPneMu #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4414</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cOMuiOk3mP #prebreak #best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4393</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/s4PNIhJQX7 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4392</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4404</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/6AqrNanKFD #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4407</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gexHzU1VK8 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4420</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/UMgD92wLjA #prebreak #best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4412</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/G62txymzBv #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4408</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/MIs0RjxuIr #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4391</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gUJNPLJVvt #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4405</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/8JcYXhq1AZ #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                         text  target\n",
              "4415  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/xV3D9bPjHi #prebreak #best  1     \n",
              "4400  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/aAtt5aMnmD #prebreak #best  0     \n",
              "4399  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/nQiObcZKrT #prebreak #best  0     \n",
              "4403  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/qj3PVgaVN7 #prebreak #best  1     \n",
              "4397  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J5onxFwLAo #prebreak #best  0     \n",
              "4396  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/wvTPuRYx63 #prebreak #best  0     \n",
              "4394  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cx6auPneMu #prebreak #best  0     \n",
              "4414  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cOMuiOk3mP #prebreak #best  1     \n",
              "4393  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/s4PNIhJQX7 #prebreak #best  0     \n",
              "4392  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best  1     \n",
              "4404  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/6AqrNanKFD #prebreak #best  0     \n",
              "4407  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gexHzU1VK8 #prebreak #best  0     \n",
              "4420  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/UMgD92wLjA #prebreak #best  1     \n",
              "4412  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/G62txymzBv #prebreak #best  0     \n",
              "4408  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/MIs0RjxuIr #prebreak #best  0     \n",
              "4391  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gUJNPLJVvt #prebreak #best  0     \n",
              "4405  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/8JcYXhq1AZ #prebreak #best  0     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpO-sZ6oo6Zt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "02e6b527-e832-4ad7-c41d-4214bc41d708"
      },
      "source": [
        "train.loc[indices,'target']=0\n",
        "train.loc[indices]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4415</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/xV3D9bPjHi #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4400</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/aAtt5aMnmD #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4399</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/nQiObcZKrT #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4403</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/qj3PVgaVN7 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4397</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J5onxFwLAo #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4396</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/wvTPuRYx63 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4394</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cx6auPneMu #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4414</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cOMuiOk3mP #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4393</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/s4PNIhJQX7 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4392</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4404</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/6AqrNanKFD #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4407</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gexHzU1VK8 #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4420</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/UMgD92wLjA #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4412</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/G62txymzBv #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4408</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/MIs0RjxuIr #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4391</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gUJNPLJVvt #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4405</th>\n",
              "      <td>#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/8JcYXhq1AZ #prebreak #best</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                         text  target\n",
              "4415  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/xV3D9bPjHi #prebreak #best  0     \n",
              "4400  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/aAtt5aMnmD #prebreak #best  0     \n",
              "4399  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/nQiObcZKrT #prebreak #best  0     \n",
              "4403  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/qj3PVgaVN7 #prebreak #best  0     \n",
              "4397  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J5onxFwLAo #prebreak #best  0     \n",
              "4396  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/wvTPuRYx63 #prebreak #best  0     \n",
              "4394  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cx6auPneMu #prebreak #best  0     \n",
              "4414  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cOMuiOk3mP #prebreak #best  0     \n",
              "4393  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/s4PNIhJQX7 #prebreak #best  0     \n",
              "4392  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best  0     \n",
              "4404  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/6AqrNanKFD #prebreak #best  0     \n",
              "4407  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gexHzU1VK8 #prebreak #best  0     \n",
              "4420  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/UMgD92wLjA #prebreak #best  0     \n",
              "4412  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/G62txymzBv #prebreak #best  0     \n",
              "4408  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/MIs0RjxuIr #prebreak #best  0     \n",
              "4391  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/gUJNPLJVvt #prebreak #best  0     \n",
              "4405  #hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/8JcYXhq1AZ #prebreak #best  0     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUayguMMpAEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "a1c3434a-d8bc-4bd5-d590-0dc5b0980867"
      },
      "source": [
        "indices=[601,576,584,608,606,603,592,604,591,587]\n",
        "train.loc[indices]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/c0p3SEsqWm via @usatoday</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/HQsU8LWltH via @usatoday</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/qZQc8WWwcN via @usatoday</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/4M5UHeyfDo via @USATODAY</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/hrqCJdovJZ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/MqbYrAvK6h</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/pWAMG8oZj4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>#FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/S4SiCMYRmH</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>#world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  http://t.co/5zDbTktwW7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>#world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  http://t.co/wvExJjRG6E</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                 text  target\n",
              "601  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/c0p3SEsqWm via @usatoday  1     \n",
              "576  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/HQsU8LWltH via @usatoday  1     \n",
              "584  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/qZQc8WWwcN via @usatoday  0     \n",
              "608  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/4M5UHeyfDo via @USATODAY  1     \n",
              "606  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/hrqCJdovJZ                0     \n",
              "603  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/MqbYrAvK6h                1     \n",
              "592  FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/pWAMG8oZj4                1     \n",
              "604  #FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/S4SiCMYRmH               1     \n",
              "591  #world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  http://t.co/5zDbTktwW7        0     \n",
              "587  #world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  http://t.co/wvExJjRG6E        1     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01_r74m0pNL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[indices,'target']=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1xD2cEpRrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "6a6ea47f-d95e-488a-c15a-78c45755709d"
      },
      "source": [
        "indices = [3913,3914,3936,3921,3941,3937,3938,3136,3133,3930,3933,3924,3917]\n",
        "train.loc[indices]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3913</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/xxkHjySn0p http://t.co/JEVHKNJGBX</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3914</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/jCDd6SD6Qn http://t.co/9gUCkjghms</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3936</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/5xmCE6JufS http://t.co/3Zo7PX3p1V</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3921</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/mTmoIa0Oo0 http://t.co/Nn4ZtCmSRU</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3941</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/6zCfHi7Srw http://t.co/vWYkDaU1vm</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3937</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/O097vSOtxk http://t.co/I23Xy7iEjj</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3938</th>\n",
              "      <td>Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/fDSaoOiskJ http://t.co/2uVmq4vAfQ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3136</th>\n",
              "      <td>Survival Kit Whistle Fire Starter Wire Saw Cree Torch Emergency Blanket S knife  - Full reÛ_ http://t.co/2OroYUNYM2 http://t.co/C9JnXz3DXC</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>Survival Kit Whistle Fire Starter Wire Saw Cree Torch Emergency Blanket S knife  - Full reÛ_ http://t.co/cm7HqwWUlZ http://t.co/KdwAzHQTov</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3930</th>\n",
              "      <td>2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/O1SMUh2unn http://t.co/xqj6WgiuQH</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3933</th>\n",
              "      <td>2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/1QT51r5h98 http://t.co/OQH1JbUEnl</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3924</th>\n",
              "      <td>2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/ApWXS5Mm44 http://t.co/DS76loZLSu</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3917</th>\n",
              "      <td>2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/VDeFmulx43 http://t.co/yqpAIjSa5g</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                             text  target\n",
              "3913  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/xxkHjySn0p http://t.co/JEVHKNJGBX  1     \n",
              "3914  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/jCDd6SD6Qn http://t.co/9gUCkjghms  0     \n",
              "3936  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/5xmCE6JufS http://t.co/3Zo7PX3p1V  0     \n",
              "3921  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/mTmoIa0Oo0 http://t.co/Nn4ZtCmSRU  0     \n",
              "3941  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/6zCfHi7Srw http://t.co/vWYkDaU1vm  0     \n",
              "3937  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/O097vSOtxk http://t.co/I23Xy7iEjj  0     \n",
              "3938  Spot Flood Combo 53inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp - Full reÛ_ http://t.co/fDSaoOiskJ http://t.co/2uVmq4vAfQ  0     \n",
              "3136  Survival Kit Whistle Fire Starter Wire Saw Cree Torch Emergency Blanket S knife  - Full reÛ_ http://t.co/2OroYUNYM2 http://t.co/C9JnXz3DXC  0     \n",
              "3133  Survival Kit Whistle Fire Starter Wire Saw Cree Torch Emergency Blanket S knife  - Full reÛ_ http://t.co/cm7HqwWUlZ http://t.co/KdwAzHQTov  1     \n",
              "3930  2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/O1SMUh2unn http://t.co/xqj6WgiuQH  0     \n",
              "3933  2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/1QT51r5h98 http://t.co/OQH1JbUEnl  0     \n",
              "3924  2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/ApWXS5Mm44 http://t.co/DS76loZLSu  1     \n",
              "3917  2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM - Full reaÛ_ http://t.co/VDeFmulx43 http://t.co/yqpAIjSa5g  0     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIUZaktzpe9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[indices,'target']=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml4mPNCdpjYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "ec16fdd4-a0a8-4c3c-a504-604d66d4874f"
      },
      "source": [
        "indices=[246,270,266,259,253,251,250,271]\n",
        "train.loc[indices]\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/6LoJOoROuk via @Change</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/0fekgyBY5F via @Change</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/x2Wn7O2a3w via @Change</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/MatIJwkzbh via @Change</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/KPQk0C4G0M via @Change</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/sW1sBua3mN via @Change</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/m8MvDSPJp7 via @Change</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/SB5R7ShcCJ via @Change</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                            text  target\n",
              "246  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/6LoJOoROuk via @Change   0     \n",
              "270  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/0fekgyBY5F via @Change  0     \n",
              "266  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/x2Wn7O2a3w via @Change  0     \n",
              "259  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/MatIJwkzbh via @Change  0     \n",
              "253  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/KPQk0C4G0M via @Change   1     \n",
              "251  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/sW1sBua3mN via @Change  1     \n",
              "250  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/m8MvDSPJp7 via @Change  0     \n",
              "271  U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/SB5R7ShcCJ via @Change   1     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vm3sFCFpqI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[indices, 'target'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfbZ8HLtpyAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "682b781d-09c0-4b29-de07-7e5e75e61f49"
      },
      "source": [
        "indices = [6119,6122,6123,6131,6160,6166,6167,6172,6212,6221,6230,6091,6108]\n",
        "train.loc[indices]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6119</th>\n",
              "      <td>That horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6122</th>\n",
              "      <td>The #Tribe just keeps sinking everyday it seems faster. As for this year it's been a titanic disaster.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6123</th>\n",
              "      <td>that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6131</th>\n",
              "      <td>'Amateur Night' Actress Reprises Role for 'Siren' - HorrorMovies.ca #horror http://t.co/W9Cd6OFfcj</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6160</th>\n",
              "      <td>@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6166</th>\n",
              "      <td>Reasons I should have gone to Warped today: tony played issues showed up sleeping w sirens played attila is there issues issues issues</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6167</th>\n",
              "      <td>Thu Aug 06 2015 01:20:32 GMT+0000 (UTC)\\n#millcityio #20150613\\ntheramin sirens</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6172</th>\n",
              "      <td>my dad said I look thinner than usual but really im over here like http://t.co/bnwyGx6luh</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6212</th>\n",
              "      <td>@PianoHands You don't know because you don't smoke. The way to make taxis and buses come is to light a cigarette to smoke while you wait.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6221</th>\n",
              "      <td>I get to smoke my shit in peace</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6230</th>\n",
              "      <td>Manuel hoping for an early Buffalo snowstorm so his accuracy improves.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6091</th>\n",
              "      <td>that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6108</th>\n",
              "      <td>Do you feel like you are sinking in low self-image? Take the quiz: http://t.co/bJoJVM0pjX http://t.co/wHOc7LHb5F</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                           text  target\n",
              "6119  That horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time          1     \n",
              "6122  The #Tribe just keeps sinking everyday it seems faster. As for this year it's been a titanic disaster.                                     1     \n",
              "6123  that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time          1     \n",
              "6131  'Amateur Night' Actress Reprises Role for 'Siren' - HorrorMovies.ca #horror http://t.co/W9Cd6OFfcj                                         1     \n",
              "6160  @SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren                                                      1     \n",
              "6166  Reasons I should have gone to Warped today: tony played issues showed up sleeping w sirens played attila is there issues issues issues     1     \n",
              "6167  Thu Aug 06 2015 01:20:32 GMT+0000 (UTC)\\n#millcityio #20150613\\ntheramin sirens                                                            1     \n",
              "6172  my dad said I look thinner than usual but really im over here like http://t.co/bnwyGx6luh                                                  1     \n",
              "6212  @PianoHands You don't know because you don't smoke. The way to make taxis and buses come is to light a cigarette to smoke while you wait.  1     \n",
              "6221  I get to smoke my shit in peace                                                                                                            1     \n",
              "6230  Manuel hoping for an early Buffalo snowstorm so his accuracy improves.                                                                     1     \n",
              "6091  that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time          1     \n",
              "6108  Do you feel like you are sinking in low self-image? Take the quiz: http://t.co/bJoJVM0pjX http://t.co/wHOc7LHb5F                           1     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsE5nWCnp61j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[indices,'target']=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSYxPNOkp-Nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "3e361ac3-a355-4963-ef3f-176a673f013e"
      },
      "source": [
        "indices = [7435,7460,7464,7466,7469,7475,7489,7495,7500,7525,7552,7572,7591,7599]\n",
        "train.loc[indices]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7435</th>\n",
              "      <td>Gunshot wound #9 is in the bicep. The only one of the ten wounds that is not in the chest/torso area.  #KerrickTrial #JonathanFerrell</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7460</th>\n",
              "      <td>@IcyMagistrate ÛÓher upper armÛÒ those /friggin/ icicle projectilesÛÒ and leg from various other wounds the girl looks like a miniature moreÛÓ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7464</th>\n",
              "      <td>Crawling in my skin\\nThese wounds they will not hea</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7466</th>\n",
              "      <td>Sorrower  - Fresh Wounds Over Old Scars  (2015 Death Metal) http://t.co/L056yj2IOi http://t.co/uTMWMjiRty</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7469</th>\n",
              "      <td>@Captainn_Morgan car wreck ??</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7475</th>\n",
              "      <td>Watertown Gazette owner charged in wreck http://t.co/JHc2RT0V9F</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7489</th>\n",
              "      <td>@GeorgeFoster72 and The Wreck of the Edmund Fitzgerald</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7495</th>\n",
              "      <td>Greer man dies in wreck http://t.co/n2qZbMZuly</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7500</th>\n",
              "      <td>Omg if Cain dies i will be an emotional wreck #emmerdale</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7525</th>\n",
              "      <td>The first piece of wreckage from the first-ever lost Boeing 777 which vanished back in early March along with the 239 people on board has</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7552</th>\n",
              "      <td>Israel wrecked my home. Now it wants my land. \\nhttps://t.co/g0r3ZR1nQj</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7572</th>\n",
              "      <td>@Kirafrog @mount_wario Did you get wrecked again?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7591</th>\n",
              "      <td>Heat wave warning aa? Ayyo dei. Just when I plan to visit friends after a year.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7599</th>\n",
              "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California #iPhone users download the Earthquake app for more information http://t.co/V3aZWOAmzK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                    text  target\n",
              "7435  Gunshot wound #9 is in the bicep. The only one of the ten wounds that is not in the chest/torso area.  #KerrickTrial #JonathanFerrell               1     \n",
              "7460  @IcyMagistrate ÛÓher upper armÛÒ those /friggin/ icicle projectilesÛÒ and leg from various other wounds the girl looks like a miniature moreÛÓ  1     \n",
              "7464  Crawling in my skin\\nThese wounds they will not hea                                                                                                 1     \n",
              "7466  Sorrower  - Fresh Wounds Over Old Scars  (2015 Death Metal) http://t.co/L056yj2IOi http://t.co/uTMWMjiRty                                           1     \n",
              "7469  @Captainn_Morgan car wreck ??                                                                                                                       1     \n",
              "7475  Watertown Gazette owner charged in wreck http://t.co/JHc2RT0V9F                                                                                     1     \n",
              "7489  @GeorgeFoster72 and The Wreck of the Edmund Fitzgerald                                                                                              1     \n",
              "7495  Greer man dies in wreck http://t.co/n2qZbMZuly                                                                                                      1     \n",
              "7500  Omg if Cain dies i will be an emotional wreck #emmerdale                                                                                            1     \n",
              "7525  The first piece of wreckage from the first-ever lost Boeing 777 which vanished back in early March along with the 239 people on board has           1     \n",
              "7552  Israel wrecked my home. Now it wants my land. \\nhttps://t.co/g0r3ZR1nQj                                                                             1     \n",
              "7572  @Kirafrog @mount_wario Did you get wrecked again?                                                                                                   1     \n",
              "7591  Heat wave warning aa? Ayyo dei. Just when I plan to visit friends after a year.                                                                     1     \n",
              "7599  1.3 #Earthquake in 9Km Ssw Of Anza California #iPhone users download the Earthquake app for more information http://t.co/V3aZWOAmzK                 1     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvGHM9nCqJNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[indices,'target']=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCJxjmz-qPbr",
        "colab_type": "text"
      },
      "source": [
        "**Split training dataset**\n",
        "\n",
        "To see if the model overfits the data during training I will take a slice of the training data as a validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqrgUui-qOjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = train.tail(1500)\n",
        "train_data = train.head(6113)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YFTU6DtqiuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JThixyYKrmJy",
        "colab_type": "text"
      },
      "source": [
        "***Clean Text***\n",
        "\n",
        "As with all datasets, text based data needs a bit of cleaning to. Some common cleaning steps are :\n",
        "\n",
        "\n",
        "\n",
        "*   **Removing Noise :** so remove things that hold little meaning like URLs and html tags in the text. Punctuation is also usually removed at this stage though the Tensorflow tokenizer I use later does this by default so I leave out the logic at this step.\n",
        "*   **Removing Stopwords :** Certain words like \"a\",\"the\", and \"are\" are very common and hold little meaning for a sentence. Removing them speeds up training and helps with accuracy.\n",
        "*  **Stemming or Lemmatization :** Many words are derived from a root or stem word. For example words like \"working\" and \"worked\" stem from the word \"work\". Reverting all words to their stem can help in some tasks though for this task I found that it made very little difference.\n",
        "\n",
        "I've defined a few functions to perform this cleaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzF8Fm84s339",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_url(sentence):\n",
        "  url = re.compile(r'https?://\\S+\\www\\.\\S+')\n",
        "  return url.sub(r'', sentence)\n",
        "\n",
        "def remove_at(sentence):\n",
        "  url = re.compile(r'@\\S+')\n",
        "  return url.sub(r'', sentence)\n",
        "\n",
        "def remove_html(sentence):\n",
        "  html = re.compile(r'<.*?>')\n",
        "  return html.sub(r'', sentence)\n",
        "\n",
        "def remove_emoji(sentence):\n",
        "  emoji = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  return emoji.sub(r'', sentence)\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "  words = sentence.split()\n",
        "  words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mb3HVFFwPhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def stem_words(sentence):\n",
        "  words = sentence.split()\n",
        "  words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdQTL-0awgKA",
        "colab_type": "text"
      },
      "source": [
        "For speed I hav wrapped all of these cleaning functions into one. This is applied to all three datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfRg3STYwfFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(data):\n",
        "  data['text'] = data['text'].apply(lambda x : remove_url(x))\n",
        "  data['text'] = data['text'].apply(lambda x : remove_at(x))\n",
        "  data['text'] = data['text'].apply(lambda x : remove_html(x))\n",
        "  data['text'] = data['text'].apply(lambda x : remove_emoji(x))\n",
        "  data['text'] = data['text'].apply(lambda x : remove_stopwords(x))\n",
        "  data['text'] = data['text'].apply(lambda x : stem_words(x))\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvFVypvOw7No",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "outputId": "0ec92bc3-0408-473c-d652-0fa5cdb941d2"
      },
      "source": [
        "train_data = clean_text(train_data)\n",
        "val_data = clean_text(val_data)\n",
        "test_data = clean_text(test)\n",
        "\n",
        "train_data['text'].head().values"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['our deed reason #earthquak may allah forgiv us',\n",
              "       'forest fire near la rong sask. canada',\n",
              "       'all resid ask shelter place notifi officers. no evacu shelter place order expect',\n",
              "       '13,000 peopl receiv #wildfir evacu order california',\n",
              "       'just got sent photo rubi #alaska smoke #wildfir pour school'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMB0dezj0orU",
        "colab_type": "text"
      },
      "source": [
        "**Encode sentences**\n",
        "\n",
        "A model will not understand what to do with a string representing a sentence. Instead it needs to be converted into an array of numbers representing the sentence. This is where a tokenizer is needed. A simple tokenizer will assign a number index to every unique word so that the model can treat it like a categorical value. Fundamentally this is what Tensorflows tokenizer does at this stage of preprocessing. \n",
        "\n",
        "To achieve this I'll define a couple of functions. The first defines the tokenizer by taking all the sentences from all three datasets and assinging an index number to every word present in the sentences. All three datasets are used here to ensure the tokenizer vocabulary includes all the wors present in the tweets. I have done a small piece of analysis in the appendix section at the bottom of this notebook to show how many words are in one of the datasets but not the others.\n",
        "\n",
        "The second function uses the tokenizer to encode all the sentences into an array of index numbers representing the sentence. The second function also pads the sentences with zeros so that they are all the same size as the longest sentence in the training dataset. Note that the tokenizer reserves the index zero for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeXH1yZMxL3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_tokenizer(train_sentences, val_sentences, test_sentences):\n",
        "  sentences = pd.concat([train_sentences, val_sentences, test_sentences])\n",
        "\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "  return tokenizer\n",
        "\n",
        "def encode(sentences, tokenizer):\n",
        "  encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "  encoded_sentences = tf.keras.preprocessing.sequence.pad_sequences(encoded_sentences, padding='post')\n",
        "\n",
        "  return encoded_sentences\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnBhRS-D2hFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = define_tokenizer(train_data['text'],val_data['text'], test_data['text'])\n",
        "\n",
        "encoded_sentences = encode(train_data['text'], tokenizer)\n",
        "val_encoded_sentences = encode(val_data['text'], tokenizer)\n",
        "encoded_test_sentences = encode(test_data['text'], tokenizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsoDlWO_29uG",
        "colab_type": "text"
      },
      "source": [
        "The tokeniser provides some interesting information about the sentences  it encodes. To get the index number assinged to a word i can look up the word in the tokenizers word index(which is just a python dict with the words as keys and the indext numbers as values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2NdfNUo26Oh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a658606a-1a03-4c67-d663-492561edff44"
      },
      "source": [
        "tokenizer.word_index['disaster']\n",
        "\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "743"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b91KQ2Dd3fCJ",
        "colab_type": "text"
      },
      "source": [
        "The word index can also be used to find out how many words are in the vacab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0rSvjnm3REA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef057281-d8cc-457b-d182-76886416b857"
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d_UwO8p3lXi",
        "colab_type": "text"
      },
      "source": [
        "As well as some configurations that are used to tokenize sentences such as whether the tokenizer changes all characters to lowercase, what split it performs to get the words from the sentences and what characters it filers out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWCfXDaM32_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e7b0df45-6e96-45da-c9b4-b3a9f0c8e017"
      },
      "source": [
        "print('Lower :', tokenizer.get_config()['lower'])\n",
        "print('Split :', tokenizer.get_config()['split'])\n",
        "print('Filters :', tokenizer.get_config()['filters'])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lower : True\n",
            "Split :  \n",
            "Filters : !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3p4MNze4BPN",
        "colab_type": "text"
      },
      "source": [
        "**Import GloVe Embedding**\n",
        "\n",
        "While I could train my own owrd embedding for the model it might help to use a pre-trained word embedding. This enables me to take advantage of an embedding that has unone more rogourous training. Additionally it will also include words that I may not have in my training dataset (but may appear in the test dataset) which helps with overfitting.\n",
        "\n",
        "The first thing to do then is to load the embedding. I'll use GloVe for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kKI4UI04v1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dict = {}\n",
        "\n",
        "with open('glove.6B.100d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vectors = np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word] = vectors\n",
        "        \n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnO4oo_67gaJ",
        "colab_type": "text"
      },
      "source": [
        "To ensure the encoding of the tokenizer and the embeddings are synchronized I use the below function to update the encoded words in the embedding with the encoding from the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTt0oLbz4ZBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "23bb86c8-31e4-4730-a6c6-c21d41411a78"
      },
      "source": [
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 100))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    \n",
        "    emb_vec = embedding_dict.get(word)\n",
        "    \n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i] = emb_vec"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-b8087919b1cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0memb_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (33) into shape (100)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D83nKJ-C79OG",
        "colab_type": "text"
      },
      "source": [
        "**Define pipeline**\n",
        "\n",
        "With the sentences encoded they can now be prepared to be fed into the model. Tensorflow provides an api to format data in its own format. While data can be inserted in a more common format(such as numpy arrays), tensorflow seems to prefer its own format and provides a few handy bits of functionality as incentives.\n",
        "\n",
        "Firstly then I will convert the encoded sentences and lbels into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gRoJp6-APax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_data = tf.data.Dataset.from_tensor_slices((encoded_sentences, train_data['target'].values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDSb51npAY9q",
        "colab_type": "text"
      },
      "source": [
        "Now the data is in the tensorflow format a few handy methods can be added to imporve the training. This includes shuffling the data per training step, processing the next batch of data for training while the current batch of data is training and defining each batch as a padded batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEZSPherAnm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline(tf_data, buffer_size=100, batch_size=32):\n",
        "  tf_data = tf_data.shuffle(buffer_size)\n",
        "  tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  df_Data = tf_data.padded_batch(batch_size, padded_shapes=([None],[]))\n",
        "\n",
        "  return tf_data\n",
        "\n",
        "tf_data = pipeline(tf_data, buffer_size=1000, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqLlaFr4BEsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPINKWWjBGyA",
        "colab_type": "text"
      },
      "source": [
        "A similar pipleline is defined for the validation dataset. The difference is the lack of shuffling to speed up the validatiaon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPKgpcFoBOlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_val_data = tf.data.Dataset.from_tensor_slices((val_encoded_sentences, val_data['target'].values))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqINk2CZBWvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_pipeline(tf_data, batch_size=1):\n",
        "  tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  tf_data = tf_data.padded_batch(batch_size, padded_shapes = ([None],[]))\n",
        "\n",
        "  return tf_data\n",
        "\n",
        "tf_val_data = val_pipeline(tf_val_data, batch_size = len(val_data))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZCX3tePCHy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf_val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxVckLIOCJfb",
        "colab_type": "text"
      },
      "source": [
        "**Train Model**\n",
        "Now I am ready to define and rain the model. Firstly I will define the model:\n",
        "\n",
        "*  **Embedding layer :** To enable the model to gain an understanding of a words meaning it will need a word embedding. This embedding creates a number of generic features against each word that could represent anything from a words gender to whether it is has a positive or negative sentiment. This also enables the model to identify relationships between words. This page from Tensorflows documnetation contains a good diagram of what one might look like\n",
        "\n",
        "*  **RNN layer :** An RNN layer is a bit complex to explain in it's entirely in this notebook. Check out the course link at the top of this notebook for a full run through of the theory. Tensorflow offers all three of the major RNN layers; simple RNN, GRU and LSTM. I tried all of these layers and found that the LSTM layer worked best here.\n",
        "\n",
        "*  **Dense Layer :** This final layer takes the LSTM output and applies a class to the sentence i.e. a 1 if the sentence is reporting a real disaster or 0 if not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT4I9v_ODRrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = tf.keras.layers.Embedding(\n",
        "    len(tokenizer.word_index) +1,\n",
        "    100,\n",
        "    embeddings_initailizer = tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCrDQ4VADf4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([embedding, \n",
        "                             tf.keras.layers.SpatialDropout1D(0,2),\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout0.2, recurrent_dropout=0.2)),\n",
        "                             tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "                             ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMXPMCqMD9kW",
        "colab_type": "text"
      },
      "source": [
        "Then compile the model defining the training function (adam) and the loss function (log loss). I have also added a metrics parameter so that the models accuracy is printed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhCsF4qeEKHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "    metrics=['accuracy','Precision','Recall']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGxC9dyPEbpP",
        "colab_type": "text"
      },
      "source": [
        "To avoid the model stepping over the optimum I'll add learning rate decay logic to reduce the learning rate if the loss plateaus for two or more epochs. I'll also add early stopping if loss hasn't fallen for five epochs. This saves some processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR5tr6MEErnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks =[\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXYNyu6oE7Mk",
        "colab_type": "text"
      },
      "source": [
        "training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_uPz1jcE_E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    tf_data,\n",
        "    validation_data = tf_val_data,\n",
        "    epochs = 50,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATA02fxCFHD3",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate**\n",
        "\n",
        "Let's take a look at the models output to get an idea how it did. The quickest and easiest evaluation method is to take a look at the metrics produced by the model. The final metrics can be extracted using the evaluate method. Since this competition uses an F1 score to rank submissions it may be worth having a look at it on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZxed9hFI6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = model.evaluate(tf_val_data)\n",
        "\n",
        "precision = metrics[2]\n",
        "recall = metrics[3]\n",
        "f1 = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "print('F1 score : ' + str(f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbcbyetFxGX",
        "colab_type": "text"
      },
      "source": [
        "Additionally the metrics produced per epoch when the model was training can be visualised to get a better idea for how the trianing went."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN7XzHDPF56W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subpltos(1,4, figsize(20,5))\n",
        "\n",
        "axs[0].set_title('Loss')\n",
        "axs[0].plot(history.history['loss'], label='train')\n",
        "axs[0].plot(history.history['val_loss'], label='val')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].set_title('Accuracy')\n",
        "axs[1].plot(history.history['accuracy'], label='train')\n",
        "axs[1].plot(history.history['val_accuracy'], label='val')\n",
        "axs[1].legend()\n",
        "\n",
        "axs[2].set_title('Precision')\n",
        "axs[2].plot(history.history['Precision'], label='train')\n",
        "axs[2].plot(history.history['val_Precision'], label='val')\n",
        "axs[2].legend()\n",
        "\n",
        "axs[3].set_title('Recall')\n",
        "axs[3].plot(history.history['Recall'], label='train')\n",
        "axs[3].plot(history.history['val_Reall'],label='val')\n",
        "axs[3].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TwxepRVHKAz",
        "colab_type": "text"
      },
      "source": [
        "It is also worth having a look at which sentences the model got wrong. To do this the model needs to produce predictions for the training dataset. This involves a slightly different pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S6hqbD3HZ17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(tf_val_data)\n",
        "predictions = np.concatenate(predictions).round().astype(int)\n",
        "\n",
        "val_data['predictions'] = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXLx4Hc8HoJk",
        "colab_type": "text"
      },
      "source": [
        "First take a look at the flase positives (when the model thought there was a disaster in the tweet but there was not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBMlncN1HwN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_positives = val_data[(val_data['predictions']==1)&(val_data['target']==0)]\n",
        "\n",
        "print('Count of flase positives: ' + str(len(false_positives)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hApl14GH9vL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_positives.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4RAquIdIAZd",
        "colab_type": "text"
      },
      "source": [
        "And then do the smae with the flase negatives( when the model didnt think there was a disaster in a tweet when in fact there was)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGd7gbvoIJrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_negatives = val_data[(val_Data['predictions']==0)&(val_data['target']==1)]\n",
        "\n",
        "print('count of false negatives: ' + str)len(false_negatives)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhxJ2XXIXIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_positives.tail(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qir_96bNIcND",
        "colab_type": "text"
      },
      "source": [
        "**Submission**\n",
        "\n",
        "With the model trained it now takes a few more steps to load the test data and use the model to label the test sentences as either diaster or no disaster. First convert the data to tensorflow dataset and apply the pipeline methods. The pipleline has been adjusted slightly to account for not wanting any shuffling and the different shape of the inpu (no label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K454n1wFIfkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_test_data = tf.data.Dataset.from_tensor_slices((encoded_test_sentences))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_MR7hiCI7f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_pipeline(tf_data, batch_size=1):\n",
        "  tf_data = tf_data.prefetch(tf.data.experimental.AUTOUNE)\n",
        "  tf_data = tf_Data.padded_batch(batch_size, padded_shapes=([None]))\n",
        "\n",
        "  return tf_data\n",
        "\n",
        "tf_test_data = test_pipeline(tf_test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxoB0Nr0JO2I",
        "colab_type": "text"
      },
      "source": [
        "Then use the model to apply labels to the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3FF1lFMJSO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(tf_test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPna_841JWUq",
        "colab_type": "text"
      },
      "source": [
        "The model outputs a probability per sentence. The easy way to set a threshold of 0.5(i.e. if the probability is less than 0.5 set the label to 0 and vice versa) is to use the round method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FMX1KYIJiVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.concatentate(predictions).round().astype(int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3GwQOQGJnhS",
        "colab_type": "text"
      },
      "source": [
        "Write the submission to a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU8kZCvWJqRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame(data={'target'}: predictions, index=test_data['id'])\n",
        "submission.index = submission.index.rename('id')\n",
        "submission.to_csv('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SWGyab5J8Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}